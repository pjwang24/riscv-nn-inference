# Improvement Log — 2026-02-16

## Feature: Separate DMA Path & Batched Inference

**Start of Day:** 6,698,993 cycles  
**Current Result:** 2,914,655 cycles (**−56.5%**, 2.30× speedup vs V1 Accelerator)

**Cumulative Speedup vs Baseline:** **16.8×** (48.9M → 2.9M)

---

### Strategy E: Separate 128-bit DMA & Batching

We identified that the accelerator's performance was bottlenecked by:
1.  Sharing the 32-bit memory port with the CPU.
2.  Reloading the same weights for every single image.

#### Changes

1.  **Separate 128-bit DMA Path**
    *   Widened `no_cache_mem` to expose a dedicated 128-bit read-only port.
    *   Accelerator now loads 128 bits (4 words) per cycle, effectively saturating the memory bandwidth.
    *   Bypasses the CPU's 32-bit bus entirely.

2.  **Batched Inference (Batch=4)**
    *   Software processes 4 images at a time.
    *   Weights for a tile are loaded **once** (wide 128-bit load).
    *   The same weights are used to compute 4 input vectors sequentially.
    *   Significantly reduces memory traffic for weights (75% reduction).

3.  **128-bit Data Consumption**
    *   Inputs are also consumed in 128-bit chunks, feeding all 4 parallel lanes simultaneously.

#### Verification
Passed with 100% accuracy on 100 MNIST images.

```
Loaded 11558 lines from inference.hex
*** PASSED *** after 2914655 simulation cycles
Total cycles: 2914655
```

---

### Strategy F: 4-Stage Pipeline + Decomposed ALU

We expanded the CPU from a 3-stage (IF-DX-WB) to a **4-stage pipeline (IF-ID-EX-WB)** and decomposed the monolithic ALU into parallel functional units.

#### Motivation
1.  **Split Critical Path**: The original DX stage was extremely long (Decode → RegRead → Mux → ALU → Branch), limiting maximum clock frequency (fMAX).
2.  **ALU Decomposition**: Replaced an 11-case deep mux with 4 parallel units + shallow 4:1 mux to further reduce logic depth.
3.  **Future-Proofing**: Prepared the pipeline structure for potential OoO or superscalar extensions.

#### Trade-offs
-   **Cycle Count**: **+3.2%** increase (2.91M → 3.01M cycles).
    *   **Branch Penalty**: Misprediction cost increased from 1 cycle → 2 cycles.
    *   **Data Hazards**: Load-use hazards now require a 1-cycle bubble (previously absorbed in long DX stage).
-   **Clock Frequency**: Although not simulated here, the critical path reduction should allow for a significant fMAX increase (e.g., +20-30%), resulting in a net **Execution Time (Time = Cycles × Period)** reduction.

#### Verification
Passed with 100% accuracy on MNIST.

```
Loaded 11558 lines from inference.hex
*** PASSED *** after 3007447 simulation cycles
Total cycles: 3007447
```

### Sky130 Synthesis Results (Preliminary)

We synthesized the `Riscv151` core using OpenLane (Sky130 PDK).

-   **Logic Cells**: ~65,912 gates
-   **Flip-Flops**: 15,570 FFs
-   **Analysis**:
    *   The high Flip-Flop count is due to the **Branch Prediction tables (BHT/BTB)** and **Register File** being synthesized as distributed RAM (Flip-Flops) rather than SRAM macros.
    *   Standard Cells: `sky130_fd_sc_hd__dfxtp_2` (D-Flip-Flop) count is 15,570.
    *   **Final Status**: PAR run `v3` (2mm x 2mm) **failed** during Detailed Routing (step 14) due to memory overflow/congestion.
    *   **Preliminary PPA (Post-Placement)**:
        *   **Area**: 4mm² Die (24% utilization). logic density is low, but routing congestion is high.
        *   **Timing**: WNS is -64.62ns (Target 10ns). Estimated Fmax ≈ **13.4 MHz**.
        *   **Conclusion**: The distributed Register/BHT arrays create massive routing congestion and critical paths. **Hard SRAM macros are mandatory** for a viable RISC-V ASIC at this node.



