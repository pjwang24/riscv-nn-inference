# Improvement Log — 2026-02-16

## Feature: Separate DMA Path & Batched Inference

**Start of Day:** 6,698,993 cycles  
**Current Result:** 2,914,655 cycles (**−56.5%**, 2.30× speedup vs V1 Accelerator)

**Cumulative Speedup vs Baseline:** **16.8×** (48.9M → 2.9M)

---

### Strategy E: Separate 128-bit DMA & Batching

We identified that the accelerator's performance was bottlenecked by:
1.  Sharing the 32-bit memory port with the CPU.
2.  Reloading the same weights for every single image.

#### Changes

1.  **Separate 128-bit DMA Path**
    *   Widened `no_cache_mem` to expose a dedicated 128-bit read-only port.
    *   Accelerator now loads 128 bits (4 words) per cycle, effectively saturating the memory bandwidth.
    *   Bypasses the CPU's 32-bit bus entirely.

2.  **Batched Inference (Batch=4)**
    *   Software processes 4 images at a time.
    *   Weights for a tile are loaded **once** (wide 128-bit load).
    *   The same weights are used to compute 4 input vectors sequentially.
    *   Significantly reduces memory traffic for weights (75% reduction).

3.  **128-bit Data Consumption**
    *   Inputs are also consumed in 128-bit chunks, feeding all 4 parallel lanes simultaneously.

#### Verification
Passed with 100% accuracy on 100 MNIST images.

```
Loaded 11558 lines from inference.hex
*** PASSED *** after 2914655 simulation cycles
Total cycles: 2914655
```
