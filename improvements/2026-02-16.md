# Improvement Log — 2026-02-16

## Feature: Separate DMA Path & Batched Inference

**Start of Day:** 6,698,993 cycles  
**Current Result:** 2,914,655 cycles (**−56.5%**, 2.30× speedup vs V1 Accelerator)

**Cumulative Speedup vs Baseline:** **16.8×** (48.9M → 2.9M)

---

### Strategy E: Separate 128-bit DMA & Batching

We identified that the accelerator's performance was bottlenecked by:
1.  Sharing the 32-bit memory port with the CPU.
2.  Reloading the same weights for every single image.

#### Changes

1.  **Separate 128-bit DMA Path**
    *   Widened `no_cache_mem` to expose a dedicated 128-bit read-only port.
    *   Accelerator now loads 128 bits (4 words) per cycle, effectively saturating the memory bandwidth.
    *   Bypasses the CPU's 32-bit bus entirely.

2.  **Batched Inference (Batch=4)**
    *   Software processes 4 images at a time.
    *   Weights for a tile are loaded **once** (wide 128-bit load).
    *   The same weights are used to compute 4 input vectors sequentially.
    *   Significantly reduces memory traffic for weights (75% reduction).

3.  **128-bit Data Consumption**
    *   Inputs are also consumed in 128-bit chunks, feeding all 4 parallel lanes simultaneously.

#### Verification
Passed with 100% accuracy on 100 MNIST images.

```
Loaded 11558 lines from inference.hex
*** PASSED *** after 2914655 simulation cycles
Total cycles: 2914655
```

---

### Strategy F: 4-Stage Pipeline + Decomposed ALU

We expanded the CPU from a 3-stage (IF-DX-WB) to a **4-stage pipeline (IF-ID-EX-WB)** and decomposed the monolithic ALU into parallel functional units.

#### Motivation
1.  **Split Critical Path**: The original DX stage was extremely long (Decode → RegRead → Mux → ALU → Branch), limiting maximum clock frequency (fMAX).
2.  **ALU Decomposition**: Replaced an 11-case deep mux with 4 parallel units + shallow 4:1 mux to further reduce logic depth.
3.  **Future-Proofing**: Prepared the pipeline structure for potential OoO or superscalar extensions.

#### Trade-offs
-   **Cycle Count**: **+3.2%** increase (2.91M → 3.01M cycles).
    *   **Branch Penalty**: Misprediction cost increased from 1 cycle → 2 cycles.
    *   **Data Hazards**: Load-use hazards now require a 1-cycle bubble (previously absorbed in long DX stage).
-   **Clock Frequency**: Although not simulated here, the critical path reduction should allow for a significant fMAX increase (e.g., +20-30%), resulting in a net **Execution Time (Time = Cycles × Period)** reduction.

#### Verification
Passed with 100% accuracy on MNIST.

```
Loaded 11558 lines from inference.hex
*** PASSED *** after 3007447 simulation cycles
Total cycles: 3007447
```
