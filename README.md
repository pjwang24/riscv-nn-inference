# Bare-Metal Neural Network Inference on a Custom RISC-V Processor

> End-to-end ML inference on a **4-stage pipelined RV32IM core** with a custom matmul accelerator, verified via cycle-accurate Verilator simulation.

## Key Results

| Metric | Value |
|--------|-------|
| ISA | RV32IM (Integer + Multiply/Divide) |
| Architecture | **4-Stage Pipeline** (IF, ID, EX, WB) |
| Accelerator | 4-lane DMA-based INT8 matmul engine |
| Simulation cycles | **3,007,447** (~3.0M) |
| Speedup vs. 3-stage baseline | **1.03×** (Cycle count increased slightly due to load-use stalls) |
| Speedup vs. unoptimized baseline | **16.2×** (from 48.9M) |
| Test images | 100 (MNIST handwritten digits) |
| Inference result | **PASSED** (all predictions correct, 100% accuracy) |
| Binary size | ~178 KB (code + weights + test data) |

---

## How It Works: From Training to Silicon

This section describes the end-to-end pipeline for deploying a trained neural network onto a bare-metal RISC-V processor. The process involves four stages: model training, weight quantization and export, cross-compilation, and cycle-accurate simulation.

### The Big Picture

```
 ┌─────────────┐     ┌──────────────┐     ┌────────────────┐     ┌─────────────────┐
 │  1. TRAIN   │────▶│  2. EXPORT   │────▶│  3. COMPILE    │────▶│  4. SIMULATE    │
 │  (PyTorch)  │     │  (Python→C)  │     │  (RISC-V GCC)  │     │  (Verilator)    │
 │             │     │              │     │                │     │                 │
 │ float32     │     │ int8 arrays  │     │ flat binary    │     │ clock-by-clock  │
 │ model.pth   │     │ weights.h    │     │ inference.hex  │     │ CPU execution   │
 └─────────────┘     └──────────────┘     └────────────────┘     └─────────────────┘
```

### Step 1: Train the Model (Python / PyTorch)

A 2-layer MLP is trained on the MNIST handwritten digit dataset using standard PyTorch:

```
Input (784 pixels) → FC1 (128 neurons) → ReLU → FC2 (10 classes) → Argmax → digit 0–9
```

After training, the learned parameters reside in the model's **weight matrices**, which are floating-point tensors (e.g., `fc1.weight` is a 128×784 matrix of `float32` values). These weights encode the statistical patterns extracted from the training data.

### Step 2: Quantize and Export (Python → C Header Files)

Since the RISC-V target has **no floating-point unit**, the model must be converted to integer arithmetic:

1. **Quantization**: Each `float32` weight is linearly scaled and rounded to `int8` (range −128 to +127). This introduces minor precision loss but reduces storage by 4× and enables pure integer computation.

2. **Export as C arrays**: The quantized weights are serialized into C header files as constant arrays:

   ```c
   // weights.h (auto-generated by train_and_export.py)
   const int8_t fc1_weight[100352] = { -5, 0, -3, -2, 3, -1, 1, ... };
   const int32_t fc1_bias[128] = { -170, 308, 38, ... };
   ```

   The 100 test images and their ground-truth labels are similarly exported to `test_images.h`.

> **Note:** The weights are not loaded at runtime from a file system. There is no file system. Instead, the weight arrays are **compiled directly into the program binary** as constant data in the `.rodata` section.

### Step 3: Cross-Compile (RISC-V GCC → Hex File)

The inference source (`inference_bare.c`) includes the weight headers via `#include`. At compile time, the weight data becomes part of the ELF binary:

```
inference_bare.c  ─┐
weights.h          ├──▶ riscv-gcc ──▶ inference.elf ──▶ inference.bin ──▶ inference.hex
test_images.h      │         ▲
start.s           ─┘         │
                       linker script (base address 0x2000)
```

The linker script places all sections (code, weights, and test data) into a single contiguous memory region starting at address `0x2000`. The final `.hex` file is a flat memory image formatted as 128-bit hex lines.

**Binary layout:**
| Section | Content | Size |
|---------|---------|------|
| `.text` | Inference code (matmul, ReLU, argmax, etc.) | ~780 B |
| `.rodata` | FC1 weights (128×784) | ~98 KB |
| `.rodata` | FC2 weights (10×128), biases | ~2 KB |
| `.rodata` | 100 test images (100×784) | ~77 KB |

### Step 4: Simulate (Verilator)

The Verilator testbench (`sim_main.cpp`) loads the hex file directly into the processor's simulated SRAM before releasing reset:

```cpp
// Pseudocode from sim_main.cpp
for each line in inference.hex:
    memory[addr] = parse_128bit_hex(line);  // written to both icache and dcache
    addr++;
```

After loading, the CPU begins execution at address `0x2000`. From the processor's perspective, the weights are simply data at known memory addresses, analogous to firmware constants stored in ROM on an embedded system.

### Summary

The deployment process requires no operating system, no file I/O, and no dynamic memory allocation. The pipeline is:

1. Trained model weights are quantized to `int8` and exported as constant C arrays
2. These arrays are compiled into the program binary and linked at fixed addresses
3. The binary is loaded into simulated memory, and the CPU reads weights directly via load instructions
4. The matmul accelerator accesses the same weight data via DMA from the shared data memory

Fundamentally, neural network inference reduces to arithmetic on arrays. Once the weight arrays are present in memory and the code knows their dimensions, inference can execute on any processor with integer arithmetic support.

---

## Project Overview

This project implements a quantized two-layer MLP running as bare-metal C on a custom RISC-V processor with a purpose-built matmul accelerator. The system spans:

1. **Model Training**: PyTorch MLP trained on MNIST, exported with INT8 weight quantization
2. **Bare-Metal C**: Fixed-point inference code compiled with `-nostdlib` for the RV32IM target
3. **HW Accelerator**: DMA-based 4-lane INT8 matmul engine with MMIO interface
4. **RTL Design**: 3-stage pipelined CPU with hardware multiply, branch prediction, and CSR support
5. **Verilator Simulation**: Cycle-accurate verification of the full hardware-software stack

---

## Processor Architecture

### 4-Stage Pipeline

```
┌──────────┐    ┌──────────┐    ┌──────────┐    ┌───────────────────┐
│  Fetch   │───▶│  Decode  │───▶│  Execute │───▶│  Memory/Writeback │
│  (IF)    │    │  (ID)    │    │  (EX)    │    │  (WB)             │
└──────────┘    └──────────┘    └──────────┘    └───────────────────┘
  • PC gen        • Instr Dec     • ALU / Mult    • Data cache R/W
  • I-cache       • Reg Read      • Branch Comp   • Reg writeback
  • BHT/BTB       • Hazard Det    • Forwarding    • Result Mux
```

### Branch Prediction

- **256-entry BHT** with 2-bit saturating counters
- **256-entry BTB** for target address prediction
- Trains on both branches and jumps (JAL/JALR)
- Mispredict correction from the execute stage

### M-Extension (Hardware Multiply/Divide)

Single-cycle combinational unit supporting all 8 RV32M instructions:

| Instruction | Operation |
|-------------|-----------|
| `MUL` | Lower 32 bits of signed × signed |
| `MULH` / `MULHSU` / `MULHU` | Upper 32 bits (signed, mixed, unsigned) |
| `DIV` / `DIVU` | Signed / unsigned division |
| `REM` / `REMU` | Signed / unsigned remainder |

### Matmul Accelerator

A DMA-based, 4-lane, weight-stationary INT8 matmul engine mapped at `0x80000000`:

- **4 MAC lanes** with 256-word weight SRAMs, processing 4 output neurons in parallel
- **Packed INT8 dot-product**: 4 byte multiplies per clock per lane (16 MACs/cycle)
- **DMA engine** for reading weights and inputs directly from data memory
- **CPU integration** via address-decoded dcache port mux; the CPU core is unmodified

The CPU writes weight/input addresses and dimensions to MMIO registers, triggers the accelerator, polls for completion, and reads the 4 accumulated INT32 results.

### Hazard Management

A centralized `HazardUnit` handles data dependencies and control hazards:
- **Forwarding**: Bypasses EX/MEM and MEM/WB results to ID stage operands, eliminating most RAW stalls.
- **Load-Use Stalls**: Inserts a 1-cycle bubble (NOP) when a load instruction is followed immediately by a use.
- **Branch Flush**: Flushes IF and ID stages (2-cycle penalty) on misprediction.

### CSR Support

The `tohost` register (CSR `0x51E`) communicates results to the testbench:
- **Write 1** → PASSED
- **Write >1** → FAILED (value encodes error count)

---

## Memory Subsystem

### No-Cache Mode (used in this project)

| Parameter | Value |
|-----------|-------|
| Data width | 128 bits (4 × 32-bit words) |
| Depth | 2M entries |
| Capacity | 32 MB per instance |
| Access latency | **1 cycle** (guaranteed) |
| Instances | 2 (icache + dcache) |

Separate instruction and data memories with no stalls. This configuration provides a performance **upper bound** by eliminating all memory latency.

### Cache Mode (alternative configuration)

The RTL also supports a cache-based hierarchy with an arbiter and external memory model. Cache misses incur multi-cycle penalties. This mode is more realistic for ASIC targets but was not used here.

### Why No-Cache Matters for ML

Neural network weights (~177 KB) are accessed sequentially during matmul. The 128×784 FC1 weight matrix alone (~98 KB) would likely exceed typical L1 cache sizes, causing frequent capacity misses. No-cache mode isolates the CPU pipeline's throughput from memory effects.

---

## Neural Network

### Architecture

```
Input (784) ──▶ FC1 (128) ──▶ ReLU ──▶ Rescale ──▶ FC2 (10) ──▶ Argmax ──▶ digit 0–9
                 INT8×INT8     INT32     INT32→INT8   INT8×INT8
```

| Layer | Shape | Parameters | Storage |
|-------|-------|------------|---------|
| FC1 weight | 128 × 784 | 100,352 | ~98 KB (INT8) |
| FC1 bias | 128 | 128 | 512 B (INT32) |
| FC2 weight | 10 × 128 | 1,280 | ~1.3 KB (INT8) |
| FC2 bias | 10 | 10 | 40 B (INT32) |
| Test images | 100 × 784 | 78,400 | ~76.5 KB (INT8) |

### Quantization

- All weights and activations are **INT8**
- Multiply-accumulate produces **INT32** intermediates (no overflow risk)
- Dynamic rescaling between layers using `max_val`-based normalization
- No floating-point hardware required

### Bare-Metal Implementation

- Compiled with `-nostdlib -nostartfiles` (no OS, no libc)
- Custom `memset` provided in-source
- Results reported via inline assembly CSR writes
- Matmul offloaded to hardware accelerator via MMIO

---

## Build & Run

### Prerequisites

- [Verilator](https://verilator.org) ≥ 5.0
- [RISC-V GNU Toolchain](https://github.com/riscv-collab/riscv-gnu-toolchain) (`riscv64-unknown-elf-gcc`)
- Python 3 (for `bin2hex.py`)

### Quick Start

```bash
# Build everything and run simulation
make clean && make

# Or run steps individually:
make inference.hex        # Cross-compile C → ELF → binary → hex
make obj_dir/Vriscv_top   # Build Verilator simulation
make run                  # Execute simulation
```

### Expected Output

```
Loaded 11398 lines from inference.hex
*** PASSED *** after 6698993 simulation cycles
Total cycles: 6698993
```

---

## Performance Analysis

### Optimization History

| Stage | Cycles | Speedup |
|-------|--------|---------|
| Original baseline (unoptimized) | 48,886,157 | 1.00× |
| + Software optimizations (reciprocal multiply, loop unrolling, fused kernels, `-O3`) | 44,830,519 | 1.09× |
| + Pipeline improvements (jump bubble fix, larger BHT/BTB) | 44,830,278 | 1.09× |
| + **Matmul accelerator** | **6,698,993** | **7.30×** |
| + **4-Stage Pipeline** (IF, ID, EX, WB) | **3,007,447** | **16.2×** |

See [IMPROVEMENTS.md](IMPROVEMENTS.md) for detailed change-by-change analysis.

### Cycle Breakdown

At ~6.7M total cycles for 100 images (~67K cycles/image):

| Metric | Value |
|--------|-------|
| MACs per image | ~101,632 (FC1: 100,352 + FC2: 1,280) |
| Total MACs | ~10.16M |
| Effective throughput | **~0.66 cycles/MAC** |

---

## Physical Implementation (Sky130)

The 4-stage core was synthesized using the OpenLane ASIC flow (Sky130 PDK).

| Metric | Result |
|---|---|
| Logic Cells | ~65,912 gates |
| Flip-Flops | ~15,570 |
| Die Area | 4 mm² (2mm × 2mm) |
| Core Utilization | ~24% |
| Estimated Fmax | ~50 MHz (with 4KB SRAM macros) |

### Reproducing Results

To reproduce these results, you can run the flow at different stages:

**1. Pre-Synthesis Verification (Verilator)**
Verifies the RTL logic and software fallback (no timing).
```bash
make run
# Expected: *** PASSED *** after ~2.1M cycles
```

**2. Post-Synthesis Verification (Gate Level)**
Maps RTL to Sky130 standard cells and checks area/timing estimates.
```bash
cd OpenLane
make mount
./flow.tcl -design riscv_top -to synthesis
# Check logs/synthesis/2-sta.log for WNS (Worst Negative Slack)
```

**3. Full Place & Route (PAR)**
Generates the final layout (GDSII) and precise timing/power reports.
```bash
cd OpenLane
make mount
./flow.tcl -design riscv_top
# Check logs/signoff for final reports
```

---

## Project Structure

```
verilator/
├── Makefile              # Build orchestration
├── sim_main.cpp          # Verilator testbench (hex loading, reset, CSR monitoring)
├── inference_bare.c      # Bare-metal MLP inference (MMIO accelerator calls)
├── inference.ld          # Linker script (base address 0x2000)
├── bin2hex.py            # Binary-to-hex converter (128-bit width)
├── IMPROVEMENTS.md       # Optimization log with cycle measurements
├── README.md
└── riscv-accelerator/
    └── MatmulAccelerator.sv  # 4-lane DMA matmul accelerator

asic-project-fa25-golden-gates/src/
├── riscv_top.v           # Top-level module (CPU + memory + accelerator mux)
├── Riscv151.v            # CPU core (pipeline integration)
├── Fetch.sv              # Fetch stage + branch prediction
├── Execute.sv            # Execute stage (ALU, multiplier, forwarding)
├── Writeback.sv          # Writeback stage
├── Control_logic.sv      # Instruction decoder
├── Memory151.v           # Memory subsystem (cache/no-cache mux)
├── no_cache_mem.v        # Direct-mapped SRAM memory
├── Multiplier.sv         # RV32M multiply/divide unit
├── ALU.v / ALUdec.v      # ALU and ALU decoder
├── RegFile.sv            # Register file
├── BranchPrediction.v    # BHT + BTB
├── Branch_Comp.v         # Branch comparator
└── const.vh / Opcode.vh  # Constants and opcode definitions

riscv-ml-inference/
├── train/
│   └── train_and_export.py  # PyTorch training + INT8 quantization + C export
└── runtime/
    ├── weights.h             # Quantized INT8 weights and INT32 biases
    └── test_images.h         # 100 MNIST test images + expected labels
```

---

## Significance

### RISC-V for Edge ML

This project demonstrates that a minimal RISC-V core with a purpose-built accelerator can execute neural network inference at high throughput. The open ISA enables:
- **Custom accelerators**: the matmul engine achieves a 7.3× speedup without modifying the CPU core
- **Area optimization**: unused features can be removed and replaced with domain-specific hardware
- **Full-stack verification**: processor, accelerator, memory, and software are validated in a single simulation

### Hardware-Software Co-Design

The optimization results (48.9M → 6.7M cycles) illustrate the relative impact of different optimization strategies:
- Software optimizations alone yielded an 8% cycle reduction
- Pipeline and branch predictor changes contributed negligible improvement for this workload
- A dedicated matmul accelerator, targeting the computational bottleneck, achieved an **85% reduction**

---

## Future Work

### Accelerator
- **Pipelined DMA**: overlap weight loading with computation (1 cycle/word instead of 2)
- **Wider MAC array**: 8 or 16 lanes for larger tile sizes
- **Double-buffered SRAMs**: load next tile's weights while computing current tile
- **Systolic array**: 2D dataflow for higher utilization

### Hardware
- **Performance counters**: cycle/instruction/misprediction CSRs for profiling
- **Cache mode benchmarking**: measure cache miss rates for this workload

### Software
- **Tiled FC2**: handle non-multiple-of-4 output dimensions more efficiently
- **Activation pipelining**: overlap bias/ReLU/rescale with next tile's matmul

### Model
- **CNNs**: 2D convolution via im2col + matmul on the accelerator
- **CIFAR-10**: more complex image classification
- **Mixed precision**: INT16 for the first layer, INT8 for deeper layers

---

## Acknowledgments

Special thanks to **Tingyao Huang** for his contributions to the design and development of the RISC-V CPU core used in this project.
